% 
% Annual Cognitive Science Conference
% Sample LaTeX Two-Page Summary -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
%\usepackage{hyperref}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\title{Statistics as Pottery: Bayesian Data Analysis using Probabilistic Programs \\(Tutorial)}
 
\author{{\large \bf Michael Henry Tessler (mhtessler@stanford.edu)}, {\large \bf Noah D. Goodman (ngoodman@stanford.edu)}  \\
  Department of Psychology, Stanford University
  }


\begin{document}

\maketitle

\begin{abstract}

Drawing inferences from observable data is a fundamental part of science, but how should we do it?
\begin{quote}
\small
\textbf{Keywords:} 
bayesian data analysis; bayesian cognitive modeling; probabilistic programming
\end{quote}

\end{abstract}





\section{Significance}

Learning statistics is like learning pottery. 
With pottery, you can learn how to make different shapes (e.g., a bowl, a vase, a spoon) without understanding general principles. 
Another way is to learn the basic strokes of forming pottery (e.g., how to mold a curved surface, a flat surface, long pointy things). 
In this course, we are going to learn the basic strokes of statistics, and compose those strokes to make shapes you've seen before (e.g., a linear model), some shapes you've probably never seen before, and develop ideas how you would make new shapes if you needed to. 
We won't learn what tests apply to what data types but instead foster the ability to reason through data analysis. 
We will do this through the lens of Bayesian statistics, though the basic ideas will aid your understanding of classical (frequentist) statistics as well.

%Analyzing experimental data is an open field of possibilities. 
%There is no \emph{one way} to get from data to results. 
%Classical tests (t-tests, linear and logistic models) are extremely useful for many cases, but what should you do when the linear model doesn't suffice? 
%You could google for your specific question. 
%If you're lucky, some statistician or psychologist will have asked your very same question of very similar data and you can plug and play. 
%But what if the search turns up nothing?

Probability theory is the ``logic of science'' \cite{jaynes2003probability} and Bayesian data analysis (BDA) is a general-purpose, flexible data analytic approach for drawing inferences about hypotheses from data.
BDA is fundamentally different from the classical statistical framework --- Null Hypothesis Significance Testing (NHST) --- in which the scientist is often forced to adopt opaque assumptions about how the data were generated, leading to illicit inferences.
This opacity as well as the inflexibility of NHST can be partially credited for the reproducibility crisis in psychology and related fields. 
BDA provides a principled alternative, one in which intuitive conclusions are drawn from intuitive, explicit assumptions.

For many years, BDA was a specialist's methodology because it required advanced computational skills to implement and fit Bayesian models.
No longer: With the advent of \emph{probabilistic programming languages} (PPLs) and recent breakthroughs in algorithms for posterior inference, large suites of data analysis problems can be approached from the Bayesian perspective. 
Furthermore, formalizing hypotheses in simple Bayesian models is a first step towards formalizing one's theory in a model. 
A formal model provides a stricter test of one's hypothesis by making explicit the relevant assumptions in one's theory. 
Finally, by formalizing hypotheses and making explicit a set of possible experiments or experimental conditions one could run, one can automate the search for \emph{good experiments} with use of an \emph{optimal experiment design} (OED) system.

This tutorial will introduce the participant to Bayesian data analysis using probabilistic programs. 
By learning how to create simple Bayesian models in a lightweight probabilistic programming language, participants will more easily grasp the relationship between the \emph{generative process} of the data and the inferences drawn from observed data.
PPLs abstract away nuisance details and allow one to build models from scratch in few lines of code. 
This tool enhances the transparency of a model, which in turn allows the scientist to reason through the model and revising it. 

This tutorial will be of broad interest to the Cognitive Science community because it touches upon a variety of distinct but related topics in the empirical disciplines. 
Foremost, it is a tutorial in data analysis and modeling assuming no background knowledge of either Bayesian statistics or probabilisitic programming (though some programming experience will be helpful). 
Rather than specific statistic tests, we teach the basic strokes of statistics (representing the generative process of data by composing probability distributions), which are transferable to whatever domain of inquiry or data the scientist engages with. 
Second, we draw the connection between analyzing simple data-analytic models (e.g., regression models) and Bayesian cognitive models, the analysis of which can both be addressed from the Bayesian data analytic perspective.
Finally, we share recent breakthroughs in the incorporation of \emph{Optimal Experiment Design} to situations where the scientists has made explicit their hypothesis in model. 
Throughout we use the same probabilistic programming language, and show participants how to incorporate this kind of modeling into their everyday scientific workflow. 

%\section{Why is this of interest to CogSci?}


\section{Learning Goals}

By the end of the tutorial, participants will be able to 

\begin{enumerate}
\tightlist
\item \textbf{Build} Bayesian statistical models for simple and complex problems using a probabilistic programming language 
\item \textbf{Interpret} the various components of such a model in terms of one's scientific hypotheses 
\item \textbf{Relate} Bayesian model to more orthodox statistical tests (e.g., a linear model) 
\item \textbf{Defend} a particular model specification (priors, likelihood) in a way that other cognitive scientists will understand
\end{enumerate}

\section{Structure}

This one-day hands-on tutorial will introduce participants to Bayesian Data Analysis, providing a set of tools and techniques that will allow researchers to conduct BDA on their own. 
We will use the probabilistic programming language WebPPL \cite{dippl}, which is freely available to run in the web browser or on the command line, as well as via the programming language R. 

\begin{enumerate}
\tightlist
\item Introduction to probabilistic programming
	\subitem Theory: Basics of probability, conditional probability
	\subitem Practical: Introduction to WebPPL and RWebPPL, creating arbitrary random variables with programs
\item Simple Bayesian data analysis
	\subitem Theory: Bayes rule 
	\subitem Practical: Posterior inference in WebPPL, comparing models
\item Joint inference models
	\subitem Theory: Representing the generative process of data by a series of random choices
	\subitem Practical: Modeling data contamination, participant-level parameters
\item Analyzing cognitive models
	\subitem Theory: Cognitive models as a more constrained kind of model
	\subitem Practical: Learning about the parameters of a cognitive model
\item Optimal Experiment Design
	\subitem Theory: Representing multiple hypotheses as models 
	\subitem Practical: Running \texttt{webppl-oed} on models
\end{enumerate}

Each participant will need a laptop but no additional materials are required for this tutorial. 

\section{Organizer Credentials}

Tessler has written tutorials and lectured on Bayesian Data Analysis and Bayesian cognitive modeling in graduate-level university classes\footnote{\url{https://probmods.org/chapters/14-bayesian-data-analysis.html}}
\footnote{\url{http://forestdb.org/models/bayesian-data-analysis.html}}.
In addition, Tessler has designed and taught his own 10-week graduate-level course on Bayesian Data Analysis at Stanford \footnote{
\url{https://web.stanford.edu/class/psych201s/}
} and 1- to 2-week summer schools around the world\footnote{
e.g., \url{http://stanford.edu/~mtessler/short-courses/2017-computational-pragmatics/}}.
Tessler's research integrates Bayesian Data Analysis with Bayesian Cognitive Modeling and has co-authored a paper on Optimal Experiment Design using probabilistic programs \cite{OuyangTLG16}.  
Tessler has also developed an R package for interfacing with WebPPL \footnote{\url{https://github.com/mhtess/rwebppl}}.
Goodman developed the probabilisitic programming language (PPL) WebPPL, as as well as two other PPLS: Church \cite{church} and Pyro\footnote{\url{http://pyro.ai}}.
He teaches courses on probabilistic models of cognition.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{tutorial}


\end{document}
