% 
% Annual Cognitive Science Conference
% Sample LaTeX Two-Page Summary -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{color}


\usepackage[scaled=0.8]{inconsolata}

\usepackage{listings}
%\usepackage{hyperref}

\newcommand{\ndg}[1]{\textcolor{green}{[ndg: #1]}}
\newcommand{\mht}[1]{\textcolor{blue}{[mht: #1]}}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
  
  
  \newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}
  
  \lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{white},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=none,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}




\title{Statistics as Pottery: Bayesian Data Analysis using Probabilistic Programs \\(Tutorial)}
 
\author{{\large \bf Michael Henry Tessler (mhtessler@stanford.edu)}, {\large \bf Noah D. Goodman (ngoodman@stanford.edu)}  \\
  Department of Psychology, Stanford University
  }


\begin{document}

\maketitle

\begin{abstract}

%Drawing inferences from observable data is a fundamental part of science, but how should we do it?
Probability theory is the ``logic of science'' \cite{jaynes2003probability} and Bayesian data analysis (BDA) is the glue that brings that logic to data.
BDA is a general, flexible alternative to standard statistical approaches (e.g., NHST) that provides the scientist with clarity and ease to address their personal scientific questions. 
Doing BDA in a probabilistic programming language (PPL) affords several additional advantages:  a compositional approach to writing models, separation of model specification from algorithmic implementation (a la \lstinline{lm()} in R), and continuity from articulating data analytic models to Bayesian cognitive models. 
Furthermore, specifying one's model and data analysis in a PPL allows you to search for ``optimal experiments'' for free. 
This tutorial will walk the participant through the basics of BDA to state-of-the-art applications, using an interactive online web-book and tools for integrating BDA into their existing workflow. 
\begin{quote}
\small
\textbf{Keywords:} 
bayesian data analysis; bayesian cognitive modeling; probabilistic programming
\end{quote}

\end{abstract}





\section{Significance}

Learning statistics is like learning pottery. 
With pottery, you can learn how to make different shapes (e.g., a bowl, a vase, a spoon) without understanding general principles. 
Another way is to learn the basic strokes of forming pottery (e.g., how to mold a curved surface, a flat surface, long pointy things). 
In this course, we are going to learn the basic strokes of statistics by building generative models of data. 
We'll compose these strokes to make shapes you've seen before (e.g., a linear model), some shapes you've probably never seen before (e.g., a single model of a two-alternative forced-choice task and a three-alternative task), and explore how you would make new shapes if you needed to. 
We won't learn what tests apply to what data types but instead foster the ability to reason through data analysis. 
We will do this through the lens of Bayesian statistics, though the basic ideas will aid your understanding of classical (frequentist) statistics as well.

%\ndg{i like the above and below paragraphs, but the transition is a bit jarring.}

%Analyzing experimental data is an open field of possibilities. 
%There is no \emph{one way} to get from data to results. 
%Classical tests (t-tests, linear and logistic models) are extremely useful for many cases, but what should you do when the linear model doesn't suffice? 
%You could google for your specific question. 
%If you're lucky, some statistician or psychologist will have asked your very same question of very similar data and you can plug and play. 
%But what if the search turns up nothing?

Bayesian data analysis (BDA) is a general-purpose, flexible data analytic approach for drawing inferences about hypotheses from data.
Recent years have elucidated the shortcomings of the ``classical'' statistical framework---Null Hypothesis Significance Testing (NHST). 
NHST is opaque and rife with assumptions the scientist is often forced to adopt, which is partially credited for the reproducibility crises in psychology and related fields. 
BDA provides a principled alternative to NHST: Intuitive conclusions are drawn from intuitive, explicit assumptions.
Major journals have made public their openness to alternatives to NHST \cite<e.g.,>{lindsay2015replication}, and Bayesian techniques are precisely that alternative.

%Drawing inferences from data is the purview of statistics, and probability theory can be seen as the ``logic of science'' \cite{jaynes2003probability}.

%BDA is fundamentally different from the classical statistical framework --- Null Hypothesis Significance Testing (NHST) --- in which the scientist is often forced to adopt opaque assumptions about how the data were generated, leading to illicit inferences.
%This opacity as well as the inflexibility of NHST can be partially credited for the reproducibility crisis in psychology and related fields. 

%\ndg{ need to say a little more here about what PPLs are, and why they reduce the need for statistical expertise.}
For many years, BDA was a specialist's methodology because it required advanced computational skills to implement and fit Bayesian models.
No longer: \emph{Probabilistic programming languages} (PPLs) abstract away nuisance details and allow one to build models from scratch in few lines of code; they further provide black-box inference algorithms that can be used with little expertise.
PPLs are programming languages have special operators for representing probabilities and randomness. 
Crucially, they provide an abstraction barrier between \emph{model specification} (\emph{what} we wish to compute) and the algorithm for returning the result (\emph{how} we try to compute it), analogous to how the function \lstinline{lm()} (in the programming language R) runs a linear model: The user simply specifies the functional form of the regression model (e.g., additive, interactive,...) and, under the hood, computation is done to return the answer.
The PPL approach enhances the transparency of a model, which in turn allows the scientist to reason through and revise the model more easily. 

PPLs are more than just a single model, however, they are full-blown programming languages that allow you to build \emph{any} model you might need.
Strange data types or experiment designs that violate assumptions of classical tests are no problem; all you have to do is be explicit about how you believe the data were generated (e.g., in the simplest case, that two experimental conditions gave rise to potentially different distributions of responses).
If your hypothesis can be made more precise, PPLs provide an avenue to formalize it in a cognitive model. 
Have a cognitive model in hand but can't think of a way to precisely integrate it with the raw behavioral data? 
You could build a simple regression model to take your predictors (predictions of the cognitive model) into the appropriate data space.
All of this can be achieved without leaving the same language.
This kind of open-ended model specification cannot be achieved using probabilistic libraries or restricted probabilistic languages (e.g, STAN, BUGS). 

State-of-the-art data analytic technologies are to be found in PPLs. 
For example, with a formal model in hand, one can automatically search for \emph{good experiments} to run (i.e., experiments that, regardless of their outcome, strongly update your beliefs about a scientific question) using ``optimal experiment design'' (OED).
OED searches from a space of possible experiments for those that distinguish a scientist's alternatives models.
OED has been implemented in the PPL that this course will use \cite{ouyangArxivOED}, and we will introduce the system at the end of the course.

%You can even use BDA to integrate a cognitive model and join it with other 
%For example, if your data is  concrete way of mapping that cognitive model to the actual behavioral data collected: You can build a cognitive model in a PPL and then write a regression model to take your predictors (predictions of the cognitive model) into the appropriate data space, all without leaving the same language.
%
%Full probabilistic programming languages, like the kind used in this course, allow for open-ended model specificathave significant advantages over libraries and more restricted probabilistic languages (e.g., STAN, BUGS), which constrain the kinds of models the user can write (e.g., no recursion) or the ways in which one can write them. 
%

%$By learning how to create simple Bayesian models in a lightweight PPL, participants will more easily grasp the relationship between the \emph{generative process} of the data and the inferences drawn from observed data.


%; all that is required is for the user to articulate a model or set of models and describe their BDA approach (e.g., model comparison) and OED comes for free.

% can declare a set of experiments or experimental conditions are considering running and use the PPL to 
%The search for optimal experiments can be 
%Once a user has articulated a model or set of models and their BDA approach (e.g., model comparison), OED can be utilized without any further specification \cite{ouyangArxivOED}, and we will introduce the topic at the end of the course.
%Indeed, OED has been implemented in the probabilistic programming language that this course will use \cite{ouyangArxivOED}, and we will introduce the topic at the end of the course.


%This grants the scientist more complete understanding of their models by learning about the model parameters in a way that represents uncertainty about our knowledge of the parameters and provides a principled way of doing model comparison, automatically taking into account model complexity by something known as Bayes' Occams' Razor.
%\ndg{this par a little dense / awkward}



% \ndg{say a little more about OED ad how it's basically free from the PPL BDA setup.}

%\ndg{be a little more explicit here about how doing BDA in a full PPL (not stan) allows BDA of bayesian cognitive models, and why that's good.}
%Furthermore, formalizing hypotheses in simple Bayesian models is a first step towards formalizing one's theory in a model. 
%A formal model provides a stricter test of one's hypothesis by making explicit the relevant assumptions in one's theory. 


This tutorial will be of broad interest to the Cognitive Science community because it touches upon a variety of distinct but related topics in the empirical disciplines. 
Foremost, it is a tutorial in modern data analysis and modeling, assuming no background knowledge of either Bayesian statistics or probabilistic programming. 
By learning how to create simple Bayesian models in a PPL, participants will more easily grasp the relationship between the \emph{generative process} of the data and the inferences drawn from observed data.
%Rather than specific statistic tests, we teach the basic strokes of statistics (representing the generative process of data by composing probability distributions), which are transferable to whatever domain of inquiry or data the scientist engages with.
Second, we draw the connection between simple data-analytic models (e.g., regression models) and Bayesian cognitive models, fostering an integrative theoretical view of data analysis and modeling.
%, the analysis of which can both be addressed from the Bayesian data analytic perspective.
Finally, we demonstrate the power of explicit data modeling by showing how Optimal Experiment Design comes for free once a Bayesian data analysis has been chosen.
Throughout, we show participants how to incorporate this kind of modeling into their everyday scientific workflow. 
%\section{Learning Goals}
%
By the end of the tutorial, participants will be able to 

\begin{enumerate}
\tightlist
\item \textbf{Build} Bayesian statistical models for simple and complex problems using a probabilistic programming language 
\item \textbf{Interpret} the various components of such a model in terms of one's scientific hypotheses 
\item \textbf{Relate} Bayesian models to more orthodox statistical models (e.g., a linear model) 
\item \textbf{Defend} a particular model specification (priors, likelihood) in a way that other cognitive scientists will understand
\end{enumerate}

\section{Structure}

This one-day hands-on tutorial will introduce participants to Bayesian Data Analysis, providing a set of tools and techniques that will allow researchers to conduct BDA on their own. 
We will use the probabilistic programming language WebPPL \cite{dippl}, which is freely available to run in the web browser or on the command line, as well as via the programming language R,\footnote{
\url{https://github.com/mhtess/rwebppl}
} to examine data sets from behavioral experiments.
The course will be taught from interactive course notes that will be posted on the web. 
A schematic website can be found at \url{https://mhtess.github.io/bdappl/}

\begin{itemize*}
\tightlist
\item Introduction to probabilistic programming
	\begin{itemize*}
	\item Theory: Basics of probability, conditional probability
	\item Introduction to WebPPL / RWebPPL, 
	\item Building generative models with programs
	\end{itemize*}
\item Simple Bayesian data analysis
	\begin{itemize*}
	\item Theory: Bayes' rule
	\item Bayesian inference in WebPPL
	\item Highest posterior density intervals
	\item Model comparison (e.g., Bayes Factors)
	\end{itemize*}
\item Joint inference models
	\begin{itemize*}
	\item Modeling data contamination
	\item Building custom regression models from scratch 
	\item Hierarchical modeling  / participant-level parameters
	\end{itemize*}
\item Analyzing cognitive models
	\begin{itemize*}
	\item Theory: Cognitive models as model of a hypothesis
	\item Learning about the parameters of a cognitive model
	\end{itemize*}
\item Optimal Experiment Design
	\begin{itemize*}
	\item Theory: Representing multiple hypotheses as models 
	\item Practical: Running \texttt{webppl-oed} on models
	\end{itemize*}
\end{itemize*}

Each participant will need a laptop but no additional materials are required for this tutorial. 

\section{Organizer Credentials}

Tessler and Goodman are experts in Bayesian cognitive modeling and use BDA extensively in their own research. 
Tessler is a 5th year PhD student (\emph{expected graduation June 2018}) who has designed and taught a 10-week graduate-level course on Bayesian Data Analysis at Stanford,\footnote{
\url{https://web.stanford.edu/class/psych201s/}} 1- to 2-week courses on Bayesian Data Analysis and probabilistic programming at the European Summer School for Logic, Language, and Information (ESSLLI), the Fall School for Computational Linguistics (German Linguistics Society), the Probabilistic Programming and Machine Learning Summer School (PPAML), and has been invited to teach a one-week course on these topics at ESSLLI in 2018.\footnote{\url{http://stanford.edu/~mtessler/short-courses/2017-computational-pragmatics/}}
He has co-authored an interactive web-book for probabilistic modeling of language \cite{problang}, a chapter on Bayesian Data Analysis,\footnote{\url{https://probmods.org/chapters/14-bayesian-data-analysis.html}} and an R package for interfacing with WebPPL.

Goodman is an Associate Professor in the Departments of Psychology and Computer Science at Stanford. 
He has taught Bayesian modeling for roughly a decade and co-authored \texttt{probmods.org}, a highly accessible interactive web-book, now in its 2nd edition, that is used for teaching Bayesian modeling at universities and institutes around the world \cite{probmods2}. 
Goodman developed the probabilistic programming language WebPPL, as well as two other PPLs: Church \cite{church} and Pyro.

%Tessler co-authored the chapter on Bayesian Data Analysis that is used in the probmods book\footnote{\url{https://probmods.org/chapters/14-bayesian-data-analysis.html}}, and has lectured on Bayesian Data Analysis and Bayesian cognitive modeling in Goodman's graduate-level class at Stanford.
%In addition, Tessler has 
%Tessler has co-taught 1- to 2-week courses on Bayesian Data Analysis, Bayesian language modeling, and probabilistic programming at the  European Summer School for Logic, Language, and Information (ESSLLI), the Fall School for Computational Linguistics (Germany), and the Probabilistic Programming and Machine Learning Summer School (PPAML), and has been invited to teach a one-week course on these topics at ESSLLI in 2018.\footnote{\url{http://stanford.edu/~mtessler/short-courses/2017-computational-pragmatics/}}


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{tutorial}


\end{document}
